{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a47172e-1c54-416b-a184-41d54917f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e2779ac-c3f9-4919-89ce-78d7a6281051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cristiano Ronaldo is a Portuguese professional football (soccer) player widely regarded as one of the greatest athletes in the history of the sport. Born on February 5, 1985, in Funchal, Madeira, Portugal, he began his professional career with Sporting CP before moving to play for Manchester United in 2003. During his time with Manchester United, he became a global superstar, winning multiple Premier League titles, a UEFA Champions League title, and the FIFA World Player of the Year award in 2008.\n",
      "\n",
      "In 2009, he transferred to Real Madrid for a then-world record fee, where he achieved extraordinary success, including winning four Champions League titles and several Ballon d'Or awards. He became the club's all-time top scorer before leaving Real Madrid in 2018 to join Juventus. While at Juventus, he continued to break records and won several domestic titles.\n",
      "\n",
      "In 2021, he returned to Manchester United but left the club in late 2022. In 2023, he signed with Al Nassr in Saudi Arabia, continuing his remarkable career. Ronaldo is known for his exceptional goal-scoring ability, athleticism, and versatility on the field. He has also had a significant impact off the pitch through his philanthropic efforts and massive global fan base. Additionally, he has represented the Portuguese national team in multiple international tournaments, including the UEFA European Championship and FIFA World Cup.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Who is Cristiano Ronaldo?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91eca754-6bdc-46b4-a8a1-068f2f0dc850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Poem: ---\n",
      "In the quiet glow of evening light,  \n",
      "A shadow slips through silver night,  \n",
      "With eyes like lanterns, green and wide,  \n",
      "A gentle prowler, soft and sly.  \n",
      "\n",
      "Whiskers twitch with secret tales,  \n",
      "In moonlit meadows, her dream prevails,  \n",
      "A whisper of paws on the velvet ground,  \n",
      "In the hush of night, she makes no sound.  \n",
      "\n",
      "Her fur, a tapestry of dusk and dawn,  \n",
      "A creature of mystery, a fleeting yawn,  \n",
      "In her gaze, the wisdom of ancient lands,  \n",
      "A world held gently in feline hands.  \n",
      "\n",
      "Oh, enigmatic traveler of twilight's seam,  \n",
      "In your purrs and stretches, the essence of dream,  \n",
      "A dance of grace, a quiet, warm embrace,  \n",
      "In the heart of night, you find your place.  \n"
     ]
    }
   ],
   "source": [
    "def chat_completion(prompt, model=\"gpt-4o\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    Sends a prompt to the Chat Completions API and returns the response.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt to send to the API.\n",
    "        model (str, optional): The model to use. Defaults to \"gpt-3.5-turbo\".\n",
    "        temperature (float, optional): Controls randomness. Defaults to 0.7.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response.\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example Usage:\n",
    "prompt = \"Write a short poem about a cat.\"\n",
    "poem = chat_completion(prompt)\n",
    "print(\"--- Poem: ---\")\n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b121c3d-45e4-4346-ad1a-8052dc89d6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of my last update, the Prime Minister of India is Narendra Modi. He has been in office since May 26, 2014. However, please verify with a current source as this information might have changed after my last update.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion(\"Who is the PM of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8d18b12-cbe6-4581-9901-20f34239402d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- JSON Output: ---\n",
      "```json\n",
      "{\n",
      "  \"Name\": \"Alice Wonderland\",\n",
      "  \"Age\": 22,\n",
      "  \"Occupation\": \"Student\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# 2. Chat Completions API - JSON Output\n",
    "def get_data_as_json(prompt, model=\"gpt-4o-mini\", temperature=0):\n",
    "    \"\"\"\n",
    "    Sends a prompt to the Chat Completions API and requests JSON output.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt to send to the API.\n",
    "        model (str, optional): The model to use. Defaults to \"gpt-3.5-turbo\".\n",
    "        temperature (float, optional): Controls randomness (0 for more consistent JSON).\n",
    "\n",
    "    Returns:\n",
    "        str: The model's JSON response.  (Note: You might want to parse this string into a Python dict)\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example Usage:\n",
    "prompt = \"\"\"\n",
    "Extract the following information and return it as a JSON object:\n",
    "Name: Alice Wonderland\n",
    "Age: 22\n",
    "Occupation: Student\n",
    "\"\"\"\n",
    "json_output = get_data_as_json(prompt)\n",
    "print(\"\\n--- JSON Output: ---\")\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d50d6714-24fe-41b8-9a9f-7bea768c1ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Product Description: ---\n",
      "**Product Name: Serenity Pro Noise-Canceling Headphones**\n",
      "\n",
      "**Description:**\n",
      "\n",
      "Immerse yourself in a world of sound with the Serenity Pro Noise-Canceling Headphones. Engineered for audiophiles and casual listeners alike, these headphones combine cutting-edge active noise cancellation technology with superior audio quality to create an unparalleled listening experience.\n",
      "\n",
      "**Key Features:**\n",
      "\n",
      "- **Active Noise Cancellation**: Our state-of-the-art ANC technology actively monitors and diminishes ambient noises, allowing you to focus on what you love—whether it’s your favorite music, a gripping podcast, or a crucial conference call.\n",
      "\n",
      "- **Crystal Clear Audio**: Featuring high-fidelity drivers, the Serenity Pro headphones deliver rich, balanced sound with deep bass, clear mids, and crisp highs. Every note comes to life, ensuring that you don’t just hear the music – you feel it.\n",
      "\n",
      "- **Comfort Fit Design**: Designed for long listening sessions, the plush ear cushions and adjustable headband provide exceptional comfort. Lightweight yet durable, they are perfect for commuting, traveling, or working from home.\n",
      "\n",
      "- **Extended Battery Life**: Enjoy up to 30 hours of continuous playback on a single charge. When you’re in a rush, a quick 15-minute charge gives you 5 hours of listening time, ensuring that you’re always connected.\n",
      "\n",
      "- **Smart Touch Controls**: Seamlessly navigate your playlist and manage calls with intuitive touch controls. Easily adjust volume, switch tracks, or activate your voice assistant without ever needing to reach for your device.\n",
      "\n",
      "- **Foldable Design**: The foldable construction makes Serenity Pro headphones ideal for travel. Slip them into the included carrying case and take your sonic sanctuary wherever you go.\n",
      "\n",
      "- **Bluetooth 5.0 Connectivity**: Experience seamless wireless connectivity with the latest Bluetooth technology, ensuring a stable connection and enhanced audio quality with minimal latency.\n",
      "\n",
      "**Why Choose Serenity Pro?**\n",
      "Whether you're looking to escape the hustle and bustle of daily life or simply want to enhance your audio experience, the Serenity Pro Noise-Canceling Headphones are your perfect companion. Elevate your listening experience and discover a new level of serenity today. \n",
      "\n",
      "**Available in sleek black, classic white, and vibrant blue. Order now and step into a world without limits!**\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# Part 2: General Prompting Techniques\n",
    "# -----------------------------------\n",
    "\n",
    "# 3. Text Generation with Role and Format\n",
    "def generate_text_with_role(prompt, role, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Generates text using a prompt that specifies a role for the AI.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The main instruction for text generation.\n",
    "        role (str):  The role to assign to the AI (e.g., \"You are a helpful assistant\").\n",
    "        model (str, optional): The model to use. Defaults to \"gpt-3.5-turbo\".\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": role},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example Usage:\n",
    "prompt = \"Write a product description for noise-canceling headphones.\"\n",
    "role = \"You are a marketing expert.\"\n",
    "description = generate_text_with_role(prompt, role)\n",
    "print(\"\\n--- Product Description: ---\")\n",
    "print(description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "984f721f-aab8-44a3-823b-5ec44d30c2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(Verse 1)  \\nYo, rise and grind, that’s the motto,  \\nStackin’ paper while you dodge the bravado,  \\nLate nights, early mornings, yeah, we hustle,  \\nDreams on my mind, gotta flex that muscle.  \\n\\nPut in the work, I’m talkin’ overtime,  \\nNo such thing as luck, it’s the heart and the grind,  \\nEyes on the prize, I’ve got vision like an eagle,  \\nBuilding my empire, oh, it’s a sequel.  \\n\\n(Chorus)  \\nWork hard, stay focused, watch your fortunes grow,  \\nBrick by brick, you lay your own path, you know,  \\nChasin’ dreams, no schemes, just the hustle and the flow,  \\nIf you want that wealth, gotta put in the show.  \\n\\n(Verse 2)  \\nFailure’s just a step in the long game of life,  \\nEvery setback’s a lesson, yeah, sharper than a knife,  \\nPersistence in my veins, motivation in my soul,  \\nGrinding for the riches, that’s my ultimate goal.  \\n\\nFrom the block to the top, I keep climbing,  \\nEvery challenge I face just keeps me shining,  \\nInvest in yourself, knowledge is the key,  \\nWhen you work hard for it, that wealth feels free.  \\n\\n(Chorus)  \\nWork hard, stay focused, watch your fortunes grow,  \\nBrick by brick, you lay your own path, you know,  \\nChasin’ dreams, no schemes, just the hustle and the flow,  \\nIf you want that wealth, gotta put in the show.  \\n\\n(Outro)  \\nSo here's to the dreamers, the doers, the fighters,  \\nKeep pushing forward, you’re destined for higher,  \\nWith every ounce of grit, success is in sight,  \\nBecoming rich by working hard, yeah, that’s the light.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Write something on becoming rich by working hard.\"\n",
    "role = \"You are a rapper\"\n",
    "\n",
    "generate_text_with_role(prompt, role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "406252b8-c99a-4244-8fd0-e03f47a06478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Summary: ---\n",
      "The Earth's climate is changing rapidly, leading to severe effects on ecosystems and human societies, including rising sea levels, more frequent extreme weather events, and declining biodiversity. Urgent action is needed to reduce greenhouse gas emissions and shift towards a sustainable future to combat these challenges.\n"
     ]
    }
   ],
   "source": [
    "# 4. Summarization with Context and Length Constraint\n",
    "def summarize_text(text, context, max_words=100, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Summarizes a given text with specified context and length constraints.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to summarize.\n",
    "        context (str): Context for the summarization (e.g., \"Summarize this scientific article\").\n",
    "        max_words (int, optional): Maximum words for the summary. Defaults to 100.\n",
    "        model (str, optional): The model to use. Defaults to \"gpt-3.5-turbo\".\n",
    "\n",
    "    Returns:\n",
    "        str: The summarized text.\n",
    "    \"\"\"\n",
    "    prompt = f\"{context}\\nSummarize the following text in under {max_words} words:\\n{text}\"\n",
    "    return chat_completion(prompt, model=model)\n",
    "\n",
    "# Example Usage:\n",
    "article = \"\"\"\n",
    "The Earth's climate is changing at an unprecedented rate, with significant impacts on ecosystems and human societies. Rising global temperatures are causing sea levels to rise, extreme weather events to become more frequent, and biodiversity to decline. Addressing climate change requires urgent action to reduce greenhouse gas emissions and transition to a sustainable future.\n",
    "\"\"\"\n",
    "context = \"Summarize this scientific article about climate change.\"\n",
    "summary = summarize_text(article, context)\n",
    "print(\"\\n--- Summary: ---\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "374063de-e83c-4cca-8534-98e67de51764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- French Translation: ---\n",
      "Merci.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------\n",
    "# Part 3: Advanced Prompting Techniques\n",
    "# ----------------------------------\n",
    "\n",
    "# 5. Few-shot Prompting for Translation\n",
    "def few_shot_translation(examples, text_to_translate, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Performs translation using few-shot prompting.\n",
    "\n",
    "    Args:\n",
    "        examples (list of tuples): A list of (source_text, target_text) tuples.\n",
    "        text_to_translate (str): The text to translate.\n",
    "        model (str, optional): The model to use. Defaults to \"gpt-3.5-turbo\".\n",
    "\n",
    "    Returns:\n",
    "        str: The translated text.\n",
    "    \"\"\"\n",
    "    prompt = \"Translate English to French:\\n\\n\"\n",
    "    for en, fr in examples:\n",
    "        prompt += f\"English: {en}\\nFrench: {fr}\\n\\n\"\n",
    "    prompt += f\"English: {text_to_translate}\\nFrench:\"\n",
    "    return chat_completion(prompt, model=model, temperature=0.3) # Lower temp for translation\n",
    "\n",
    "# Example Usage:\n",
    "translation_examples = [\n",
    "    (\"Hello, how are you?\", \"Bonjour, comment ça va?\"),\n",
    "    (\"Good morning.\", \"Bonjour.\"),\n",
    "    (\"What is your name?\", \"Comment t'appelles-tu?\")\n",
    "]\n",
    "text_to_translate = \"Thank you.\"\n",
    "french_translation = few_shot_translation(translation_examples, text_to_translate)\n",
    "print(\"\\n--- French Translation: ---\")\n",
    "print(french_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cac000a2-2575-457a-b831-472e58069c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Math Solution (Chain-of-Thought): ---\n",
      "To find out how long it takes for the train to cover 120 miles at a speed of 60 mph, we use the formula:\n",
      "\n",
      "\\[\n",
      "\\text{Time} = \\frac{\\text{Distance}}{\\text{Speed}}\n",
      "\\]\n",
      "\n",
      "Substitute the given values into the formula:\n",
      "\n",
      "\\[\n",
      "\\text{Time} = \\frac{120 \\text{ miles}}{60 \\text{ mph}} = 2 \\text{ hours}\n",
      "\\]\n",
      "\n",
      "Therefore, the train takes 2 hours to travel 120 miles.\n"
     ]
    }
   ],
   "source": [
    "# 6. Chain-of-Thought (CoT) Prompting for Arithmetic\n",
    "def chain_of_thought_math(problem, model=\"gpt-4o\"):\n",
    "    \"\"\"\n",
    "    Solves a math problem using Chain-of-Thought prompting.\n",
    "\n",
    "    Args:\n",
    "        problem (str): The math problem to solve.\n",
    "        model (str, optional): The model to use. Defaults to \"gpt-3.5-turbo\".\n",
    "\n",
    "    Returns:\n",
    "        str: The solution with the reasoning steps.\n",
    "    \"\"\"\n",
    "\n",
    "    cot_prompt = f\"Let's think step by step. {problem}  Therefore, the answer is:\"\n",
    "    return chat_completion(cot_prompt, model=model)\n",
    "\n",
    "# Example Usage:\n",
    "math_problem = \"If a train travels at 60 mph and covers 120 miles, how long does it take?\"\n",
    "solution = chain_of_thought_math(math_problem)\n",
    "print(\"\\n--- Math Solution (Chain-of-Thought): ---\")\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fd03bb8-0a12-45fe-a245-7e04b65b73af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Few-Shot CoT Solution: ---\n",
      "John started with 15 marbles. He lost 4 marbles, so he now has 15 - 4 = 11 marbles. Then he found 10 more marbles, so he now has 11 + 10 = 21 marbles. So the answer is 21.\n"
     ]
    }
   ],
   "source": [
    "# 7. Few-shot CoT Prompting\n",
    "def few_shot_cot_reasoning(examples, problem, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Performs reasoning with few-shot Chain-of-Thought prompting.\n",
    "    Note:  This is a simplified illustration.  Real-world few-shot CoT often requires\n",
    "           several more diverse examples to be truly effective.\n",
    "\n",
    "    Args:\n",
    "        examples (list of dicts): List of {\"question\": str, \"answer\": str} with CoT reasoning.\n",
    "        problem (str): The problem to solve.\n",
    "        model (str, optional): The model.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's reasoned solution.\n",
    "    \"\"\"\n",
    "    prompt = \"Solve the following problems step by step:\\n\\n\"\n",
    "    for ex in examples:\n",
    "        prompt += f\"Q: {ex['question']}\\nA: {ex['answer']}\\n\\n\"\n",
    "    prompt += f\"Q: {problem}\\nA:\"\n",
    "    return chat_completion(prompt, model=model)\n",
    "\n",
    "\n",
    "# Example (Simplified)\n",
    "cot_examples = [\n",
    "    {\"question\": \"Roger has 5 tennis balls. He buys 2 more cans of 3 tennis balls each. How many tennis balls does he have?\",\n",
    "     \"answer\": \"Roger started with 5 balls. 2 cans of 3 tennis balls each is 2 * 3 = 6 more balls. 5 + 6 = 11. So the answer is 11.\"},\n",
    "    {\"question\": \"Lisa has 10 cookies and gives 2 to her brother and 3 to her friend. How many cookies does she have left?\",\n",
    "     \"answer\": \"Lisa started with 10 cookies. She gave away 2 + 3 = 5 cookies. 10 - 5 = 5. So the answer is 5.\"}\n",
    "]\n",
    "new_problem = \"John has 15 marbles. He loses 4 marbles and then finds 10 more. How many marbles does he have?\"\n",
    "solution = few_shot_cot_reasoning(cot_examples, new_problem)\n",
    "print(\"\\n--- Few-Shot CoT Solution: ---\")\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93ff4fbc-79d8-47d7-8fe1-c2ae122a055c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ReAct Prompting Solution: ---\n",
      " The capital of France is Paris. This is a well-known fact and can be easily confirmed by researching online, consulting a map, or asking someone who knows about world geography.\n"
     ]
    }
   ],
   "source": [
    "# 1. ReAct Prompting\n",
    "def react_prompt(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Performs reasoning and acting using ReAct prompting.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt with the question.\n",
    "        model (str, optional): The model to use. Defaults to \"gpt-3.5-turbo\".\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response with interleaved reasoning and actions.\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example Usage:\n",
    "react_example = \"\"\"\n",
    "Solve the question by reasoning and acting.\n",
    "Question: What is the capital of France?\n",
    "\"\"\"\n",
    "react_solution = react_prompt(react_example)\n",
    "print(\"\\n--- ReAct Prompting Solution: ---\\n\", react_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "83a987ed-78ea-459c-9bba-be337bac3944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Self-Consistency Prompting Solutions: ---\n",
      " ['The result of 123 multiplied by 456 is 56,088.', '123 multiplied by 456 is equal to 56,088. \\n\\nTherefore, 123*456 = 56,088.', 'The multiplication of 123 with 456 is equal to 56,088.', '123 * 456 = 56,088', '123 * 456 = 56,088.']\n"
     ]
    }
   ],
   "source": [
    "# 2. Self-Consistency Prompting\n",
    "def self_consistency_prompt(prompt, num_samples=5, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Generates multiple responses and returns them for self-consistency checking.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt for the task.\n",
    "        num_samples (int, optional): Number of samples to generate. Defaults to 5.\n",
    "        model (str, optional): The model to use. Defaults to \"gpt-3.5-turbo\".\n",
    "\n",
    "    Returns:\n",
    "        list: A list of generated responses.\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    for _ in range(num_samples):\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "    return responses\n",
    "\n",
    "# Example Usage:\n",
    "self_consistency_example = \"Solve the following problem: What is 123*456?\"\n",
    "self_consistency_solutions = self_consistency_prompt(self_consistency_example)\n",
    "print(\"\\n--- Self-Consistency Prompting Solutions: ---\\n\", self_consistency_solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6970c56-a8cf-40a3-822b-c295b0b5ab10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- General Knowledge Prompting Solution: ---\n",
      " Mount Everest is located in Nepal.\n"
     ]
    }
   ],
   "source": [
    "# 3. General Knowledge Prompting\n",
    "def general_knowledge_prompt(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Prompts the model with general knowledge to answer a question.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt with the question and context.\n",
    "        model (str, optional): The model to use. Defaults to \"gpt-3.5-turbo\".\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response.\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example Usage:\n",
    "general_knowledge_example = \"\"\"\n",
    "Answer the question about geography.\n",
    "Question: In which country is Mount Everest located?\n",
    "\"\"\"\n",
    "general_knowledge_solution = general_knowledge_prompt(general_knowledge_example)\n",
    "print(\"\\n--- General Knowledge Prompting Solution: ---\\n\", general_knowledge_solution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f2bcace-3add-4ad9-b0ac-27f3a7df450a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Program-aided Language Models Solution: ---\n",
      " To calculate the square root of 25 in Python, you can use the `math` module, which provides a `sqrt` function. Here is a simple code snippet to do that:\n",
      "\n",
      "```python\n",
      "import math\n",
      "\n",
      "# Calculate the square root of 25\n",
      "square_root = math.sqrt(25)\n",
      "\n",
      "# Print the result\n",
      "print(\"The square root of 25 is:\", square_root)\n",
      "```\n",
      "\n",
      "When you run this code, it will output:\n",
      "\n",
      "```\n",
      "The square root of 25 is: 5.0\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# 4. Program-aided Language Models\n",
    "def program_aided_prompt(prompt, model=\"gpt-4o\"):\n",
    "    \"\"\"\n",
    "    Prompts the model to generate and execute code.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt asking for code generation and execution.\n",
    "        model (str, optional): The model to use. Defaults to \"gpt-3.5-turbo\".\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response, which may include the executed code output.\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example Usage:\n",
    "program_aided_example = \"\"\"\n",
    "Write a python code to calculate the square root of 25 and print the result.\n",
    "\"\"\"\n",
    "program_aided_solution = program_aided_prompt(program_aided_example)\n",
    "print(\"\\n--- Program-aided Language Models Solution: ---\\n\", program_aided_solution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "937e86f7-819c-4ffb-a212-db0cc9d82f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To determine if a number is prime, you can implement a Python function that checks if the number is divisible by any number other than 1 and itself. Here\\'s a simple function to achieve that:\\n\\n```python\\ndef is_prime(number):\\n    \"\"\"Return True if the number is a prime number, False otherwise.\"\"\"\\n    if number <= 1:\\n        return False\\n    if number <= 3:\\n        return True\\n    if number % 2 == 0 or number % 3 == 0:\\n        return False\\n    i = 5\\n    while i * i <= number:\\n        if number % i == 0 or number % (i + 2) == 0:\\n            return False\\n        i += 6\\n    return True\\n\\n# Example usage:\\nprint(is_prime(7))  # Output: True\\nprint(is_prime(10)) # Output: False\\n```\\n\\n### Explanation:\\n\\n1. **Initial Checks**:\\n   - If the number is less than or equal to 1, it is not prime.\\n   - Numbers 2 and 3 are prime numbers.\\n\\n2. **Efficiency Improvements**:\\n   - Eliminate even numbers and multiples of 3 early: If a number is divisible by 2 or 3 (other than 2 and 3 themselves), it\\'s not prime.\\n   \\n3. **Loop through potential factors**:\\n   - The loop starts at 5 and increments by 6 (to check numbers of the form 6k ± 1, which are potential prime candidates).\\n   - The loop checks divisibility up to the square root of the number (`i * i <= number`). This is because a larger factor of the number must be a multiple of a smaller factor that has already been checked.\\n   \\nThis approach balances performance and simplicity, providing a more efficient method than checking all numbers from 2 up to the number itself.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "program_aided_prompt(\"Write a python code function to return if a number is prime or not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8a086270-d2a1-40fd-9552-cb52617b0b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Role-Playing Prompt Example: ---\n",
      " I’m sorry to hear that your order has been delayed. I completely understand how frustrating this must be for you. Let’s work together to resolve this issue as quickly as possible. Could you please provide me with your order number and any other relevant information? This will help me look into the status of your order and provide you with an update. Thank you for your patience and understanding.\n"
     ]
    }
   ],
   "source": [
    "# 5. Role-Playing Prompting\n",
    "def role_playing_prompt(role, scenario, model=\"gpt-4o\"):\n",
    "    \"\"\"\n",
    "    Engages the model in a role-playing scenario.\n",
    "\n",
    "    Args:\n",
    "        role (str): The role the model should adopt.\n",
    "        scenario (str): The scenario for the role-play.\n",
    "        model (str, optional): The model to use. Defaults to \"gpt-3.5-turbo\".\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response in the given role.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"You are a {role}.\"},\n",
    "        {\"role\": \"user\", \"content\": scenario}\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example Usage:\n",
    "role = \"helpful customer service agent\"\n",
    "scenario = \"A customer is complaining about a delayed order. Respond to the customer.\"\n",
    "role_play_response = role_playing_prompt(role, scenario)\n",
    "print(\"\\n--- Role-Playing Prompt Example: ---\\n\", role_play_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc8404-7fce-405d-b1a2-04d8d85ce751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "python3.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
